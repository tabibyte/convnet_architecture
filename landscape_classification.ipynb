{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root='data/seg_train', transform=data_transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = datasets.ImageFolder(root='data/seg_test', transform=data_transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_SingleProcessDataLoaderIter' object has no attribute 'next'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\repo\\cnn\\main.ipynb Cell 3\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/repo/cnn/main.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Get a batch of training data\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/repo/cnn/main.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m dataiter \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(train_loader)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/repo/cnn/main.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m images, labels \u001b[39m=\u001b[39m dataiter\u001b[39m.\u001b[39;49mnext()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/repo/cnn/main.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Create a grid of images\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/repo/cnn/main.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m grid \u001b[39m=\u001b[39m make_grid(images, nrow\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, normalize\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_SingleProcessDataLoaderIter' object has no attribute 'next'"
     ]
    }
   ],
   "source": [
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "# Get a batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# Create a grid of images\n",
    "grid = make_grid(images, nrow=8, padding=2, normalize=True)\n",
    "\n",
    "# Display the grid of images\n",
    "plt.imshow(np.transpose(grid.numpy(), (1, 2, 0)))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),  # Adjusted input channels and kernel size\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # Adjusted kernel size\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.drop_out = nn.Dropout()\n",
    "\n",
    "        # Calculate the input size for the first fully connected layer based on the new image size (150x150)\n",
    "        self.fc_input_size = 64 * 37 * 37  # 64 channels, 37x37 spatial dimensions after two max-pooling layers\n",
    "\n",
    "        self.fc1 = nn.Linear(self.fc_input_size, 100)\n",
    "        self.fc2 = nn.Linear(100, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.drop_out(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "model = ConvNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 10\n",
    "lowest_val_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.1324, Validation Loss: 0.8329\n",
      "Epoch [2/10], Loss: 0.7093, Validation Loss: 0.6573\n",
      "Epoch [3/10], Loss: 1.0424, Validation Loss: 0.5606\n",
      "Epoch [4/10], Loss: 0.3018, Validation Loss: 0.4296\n",
      "Epoch [5/10], Loss: 0.5719, Validation Loss: 0.3914\n",
      "Epoch [6/10], Loss: 0.2770, Validation Loss: 0.2565\n",
      "Epoch [7/10], Loss: 0.2353, Validation Loss: 0.1876\n",
      "Epoch [8/10], Loss: 0.0116, Validation Loss: 0.2581\n",
      "Epoch [9/10], Loss: 0.3647, Validation Loss: 0.1361\n",
      "Epoch [10/10], Loss: 0.3087, Validation Loss: 0.0895\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad() \n",
    "        outputs = model(inputs) \n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()  \n",
    "        optimizer.step() \n",
    "\n",
    "    model.eval()\n",
    "    validation_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in train_loader:  \n",
    "            outputs = model(inputs) \n",
    "            validation_loss += criterion(outputs, targets).item()  \n",
    "\n",
    "    validation_loss /= len(train_loader)\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}, Validation Loss: {validation_loss:.4f}')\n",
    "\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test data: 79.18%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model.pth'))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy on the test data: {(correct / total) * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
